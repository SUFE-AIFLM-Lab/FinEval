<div align="center">
  <img src="docs/zh_cn/_static/image/FinEval.jpg" width="500px"/>
  <br />
  <br />

[![license](https://img.shields.io/badge/License-Apache--2.0-blue.svg)](https://github.com/InternLM/opencompass/blob/main/LICENSE)

[ğŸŒç½‘ç«™](https://opencompasstest1.readthedocs.io/zh_CN/latest/index.html) |
[ğŸ¤—Hugging Face](https://huggingface.co/datasets/SUFE-AIFLM-Lab/FinEval) |
[ğŸ“ƒè®ºæ–‡](https://arxiv.org/abs/2305.08322)

[English](/README.md) | ç®€ä½“ä¸­æ–‡
</div>

æ¬¢è¿æ¥åˆ°**FinEval**

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œç„¶è€Œå®ƒä»¬åœ¨æ›´å…·æŒ‘æˆ˜æ€§å’Œç‰¹å®šé¢†åŸŸä»»åŠ¡ä¸­çš„æ•ˆåŠ›ä»ç„¶å¾ˆå°‘è¢«æ¢ç´¢ã€‚æœ¬æ–‡ä»‹ç»äº†FinEvalï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºLLMsä¸­çš„é‡‘èé¢†åŸŸçŸ¥è¯†è®¾è®¡çš„åŸºå‡†æµ‹è¯•ã€‚

FinEvalæ˜¯ä¸€ä¸ªåŒ…å«**é‡‘èã€ç»æµã€ä¼šè®¡å’Œè¯ä¹¦**ç­‰é¢†åŸŸé«˜è´¨é‡å¤šé¡¹é€‰æ‹©é¢˜çš„é›†åˆã€‚å®ƒåŒ…æ‹¬äº†4,661ä¸ªé—®é¢˜ï¼Œæ¶µç›–äº†34ä¸ªä¸åŒçš„å­¦ç§‘ã€‚ä¸ºäº†ç¡®ä¿å¯¹æ¨¡å‹æ€§èƒ½çš„å…¨é¢è¯„ä¼°ï¼ŒFinEvalé‡‡ç”¨äº†é›¶æ ·æœ¬ã€å°‘æ ·æœ¬ã€ä»…ç­”æ¡ˆå’Œé“¾å¼æ€ç»´æç¤ºç­‰å¤šç§æ–¹æ³•ã€‚åœ¨FinEvalä¸Šè¯„ä¼°æœ€å…ˆè¿›çš„ä¸­æ–‡å’Œè‹±æ–‡LLMsï¼Œç»“æœæ˜¾ç¤ºåªæœ‰GPT-4åœ¨ä¸åŒçš„æç¤ºè®¾ç½®ä¸‹è¾¾åˆ°äº†70%çš„å‡†ç¡®ç‡ï¼Œè¡¨æ˜LLMsåœ¨é‡‘èé¢†åŸŸçŸ¥è¯†æ–¹é¢å…·æœ‰æ˜¾è‘—çš„å¢é•¿æ½œåŠ›ã€‚æˆ‘ä»¬çš„å·¥ä½œæä¾›äº†ä¸€ä¸ªæ›´å…¨é¢çš„é‡‘èçŸ¥è¯†è¯„ä¼°åŸºå‡†ï¼Œåˆ©ç”¨çº¸è´¨å®è·µé¢˜ç›®ï¼Œæ¶µç›–äº†å¹¿æ³›çš„LLMsè¯„ä¼°èŒƒå›´ã€‚

## ç›®å½•

- [æ€§èƒ½æ’è¡Œæ¦œ](#æ€§èƒ½æ’è¡Œæ¦œ)
  - [ä»…é¢„æµ‹ç­”æ¡ˆ](#ä»…é¢„æµ‹ç­”æ¡ˆ)
  - [æ€ç»´é“¾](#æ€ç»´é“¾)
- [å®‰è£…](#å®‰è£…)
- [è¯„æµ‹](#è¯„æµ‹)
- [å‡†å¤‡æ•°æ®é›†](#å‡†å¤‡æ•°æ®é›†)
- [æ”¯æŒæ–°æ•°æ®é›†å’Œæ¨¡å‹](#æ”¯æŒæ–°æ•°æ®é›†å’Œæ¨¡å‹)
- [å¦‚ä½•æäº¤](#å¦‚ä½•æäº¤)
- [å¼•ç”¨](#å¼•ç”¨)


## æ€§èƒ½æ’è¡Œæ¦œ

æˆ‘ä»¬åˆ†ä¸º**ä»…é¢„æµ‹ç­”æ¡ˆ**å’Œ**æ€ç»´é“¾**å¯¹æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œå¦‚æœéœ€è¦äº†è§£ä¸¤ç§æ–¹æ³•çš„Promptæ ·ä¾‹ï¼Œè¯·å‚è€ƒ[ä»…é¢„æµ‹ç­”æ¡ˆçš„zero-shot](/docs/zh_cn/prompt/zero_shot.md)ã€[ä»…é¢„æµ‹ç­”æ¡ˆçš„few-shot](/docs/zh_cn/prompt/few_shot.md)å’Œ[æ€ç»´é“¾](/docs/zh_cn/prompt/cot.md)ã€‚

ä¸‹é¢æ˜¯æ¨¡å‹çš„zero-shotå’Œfive-shotå‡†ç¡®ç‡:


### ä»…é¢„æµ‹ç­”æ¡ˆ

#### Zero-shot
| Model               | Finance | Accounting | Economy | Certificate | Average |
| ------------------- | :-----: | :--------: | :-----: | :---------: | :-----: |
| Random              | 25.0    |    25.0    |  25.0   |    25.0     |  25.0  |
| GPT-4               | 65.2 |      74.7      |    62.5    | 64.7  |  **66.4**   |
| GPT-3.5-turbo       | 49.0 |      58.0      |    48.8    | 50.4  |  51.0   |
| Baichuan-7B         | 48.5 |      58.6      |    47.3    | 50.1  |  50.5   |
| Baichuan-13B-base   | 39.1 |      53.0      |    47.7    | 42.7  |  44.3   |
| Baichuan-13B-chat   | 36.7 |      55.8      |    47.7    | 43.0  |  44.0   |
| LLaMA-7B-hf | 38.6 |      47.6      |    39.5    | 39.0  |  40.6   |
| Chinese-Alpaca-Plus-7B    | 33.3 |      48.3      |    41.3    | 38.0  |  38.9   |
| LLaMA-2-7B-base          | 32.6 |      41.2      |    34.1    | 33.0  |  34.7   |
| LLaMA-2-13B-base   | 31.6 |      37.0      |    33.4    | 32.1  |  33.1   |
| LLaMA-2-13B-chat   | 27.4 |      39.2      |    32.5    | 28.0  |  30.9   |
| LLaMA2-70B-chat    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| ChatGLM-6B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| ChatGLM2-6B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| Bloomz-7B1    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| InternLM-7B-chat    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| Ziya-LLaMA-13B-v1    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| Falcon-7B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| Falcon-40B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| Aquila-7B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| AquilaChat-7B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| moss-moon-003-base    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| moss-moon-003-sft    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |


#### Five-shot
| Model               | Finance | Accounting | Economy | Certificate | Average |
| ------------------- | :-----: | :--------: | :-----: | :---------: | :-----: |
| Random              | 25.0    |    25.0    |  25.0   |    25.0     |  25.0  |
| GPT-4               | 65.2 |      74.7      |    62.5    | 64.7  |  **66.4**   |
| GPT-3.5-turbo       | 49.0 |      58.0      |    48.8    | 50.4  |  51.0   |
| Baichuan-7B         | 48.5 |      58.6      |    47.3    | 50.1  |  50.5   |
| Baichuan-13B-base   | 39.1 |      53.0      |    47.7    | 42.7  |  44.3   |
| Baichuan-13B-chat   | 36.7 |      55.8      |    47.7    | 43.0  |  44.0   |
| LLaMA-7B-hf | 38.6 |      47.6      |    39.5    | 39.0  |  40.6   |
| Chinese-Alpaca-Plus-7B    | 33.3 |      48.3      |    41.3    | 38.0  |  38.9   |
| LLaMA-2-7B-base          | 32.6 |      41.2      |    34.1    | 33.0  |  34.7   |
| LLaMA-2-13B-base   | 31.6 |      37.0      |    33.4    | 32.1  |  33.1   |
| LLaMA-2-13B-chat   | 27.4 |      39.2      |    32.5    | 28.0  |  30.9   |
| LLaMA2-70B-chat    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| ChatGLM-6B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| ChatGLM2-6B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| Bloomz-7B1    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| InternLM-7B-chat    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| Ziya-LLaMA-13B-v1    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| Falcon-7B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| Falcon-40B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| Aquila-7B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| AquilaChat-7B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| moss-moon-003-base    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| moss-moon-003-sft    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |


### æ€ç»´é“¾

#### Zero-shot
| Model               | Finance | Accounting | Economy | Certificate | Average |
| ------------------- | :-----: | :--------: | :-----: | :---------: | :-----: |
| Random              | 25.0    |    25.0    |  25.0   |    25.0     |  25.0  |
| GPT-4               | 65.2 |      74.7      |    62.5    | 64.7  |  **66.4**   |
| GPT-3.5-turbo       | 49.0 |      58.0      |    48.8    | 50.4  |  51.0   |
| Baichuan-7B         | 48.5 |      58.6      |    47.3    | 50.1  |  50.5   |
| Baichuan-13B-base   | 39.1 |      53.0      |    47.7    | 42.7  |  44.3   |
| Baichuan-13B-chat   | 36.7 |      55.8      |    47.7    | 43.0  |  44.0   |
| LLaMA-7B-hf | 38.6 |      47.6      |    39.5    | 39.0  |  40.6   |
| Chinese-Alpaca-Plus-7B    | 33.3 |      48.3      |    41.3    | 38.0  |  38.9   |
| LLaMA-2-7B-base          | 32.6 |      41.2      |    34.1    | 33.0  |  34.7   |
| LLaMA-2-13B-base   | 31.6 |      37.0      |    33.4    | 32.1  |  33.1   |
| LLaMA-2-13B-chat   | 27.4 |      39.2      |    32.5    | 28.0  |  30.9   |
| LLaMA2-70B-chat    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| ChatGLM-6B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| ChatGLM2-6B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| Bloomz-7B1    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| InternLM-7B-chat    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| Ziya-LLaMA-13B-v1    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| Falcon-7B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| Falcon-40B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| Aquila-7B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| AquilaChat-7B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| moss-moon-003-base    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| moss-moon-003-sft    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |


#### Five-shot
| Model               | Finance | Accounting | Economy | Certificate | Average |
| ------------------- | :-----: | :--------: | :-----: | :---------: | :-----: |
| Random              | 25.0    |    25.0    |  25.0   |    25.0     |  25.0  |
| GPT-4               | 65.2 |      74.7      |    62.5    | 64.7  |  **66.4**   |
| GPT-3.5-turbo       | 49.0 |      58.0      |    48.8    | 50.4  |  51.0   |
| Baichuan-7B         | 48.5 |      58.6      |    47.3    | 50.1  |  50.5   |
| Baichuan-13B-base   | 39.1 |      53.0      |    47.7    | 42.7  |  44.3   |
| Baichuan-13B-chat   | 36.7 |      55.8      |    47.7    | 43.0  |  44.0   |
| LLaMA-7B-hf | 38.6 |      47.6      |    39.5    | 39.0  |  40.6   |
| Chinese-Alpaca-Plus-7B    | 33.3 |      48.3      |    41.3    | 38.0  |  38.9   |
| LLaMA-2-7B-base          | 32.6 |      41.2      |    34.1    | 33.0  |  34.7   |
| LLaMA-2-13B-base   | 31.6 |      37.0      |    33.4    | 32.1  |  33.1   |
| LLaMA-2-13B-chat   | 27.4 |      39.2      |    32.5    | 28.0  |  30.9   |
| LLaMA2-70B-chat    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| ChatGLM-6B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| ChatGLM2-6B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| Bloomz-7B1    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| InternLM-7B-chat    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| Ziya-LLaMA-13B-v1    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| Falcon-7B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| Falcon-40B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| Aquila-7B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| AquilaChat-7B    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| moss-moon-003-base    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |
| moss-moon-003-sft    | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |


## å®‰è£…

ä¸‹é¢å±•ç¤ºäº†å¿«é€Ÿå®‰è£…çš„æ­¥éª¤ï¼Œè¯¦ç»†è¯·å‚è€ƒ[å®‰è£…æŒ‡å—](docs/zh_cn/get_started/install.md)ã€‚

 ```python
    conda create --name fineval_venv python=3.8
    conda activate fineval_venv
 ```

```python
    git clone https://github.com/SUFE-AIFLM/FinEval
    cd FinEval
    pip install -r requirements.txt
    
    requirements.txt æ–‡ä»¶å¦‚ä¸‹:
    pandas
    torch
    tqdm
    peft 
    sentencepiece
```

## å‡†å¤‡æ•°æ®é›†

ä½¿ç”¨Hugging Face datasetsä¸‹è½½æ•°æ®é›†ã€‚è¿è¡Œå‘½ä»¤è¿›è¡Œ**æ‰‹åŠ¨ä¸‹è½½è§£å‹**ï¼Œåœ¨Fineval/codeçš„é¡¹ç›®ç›®å½•ä¸‹è¿è¡Œä¸‹é¢å‘½ä»¤ï¼Œå¹¶æ”¹åä¸ºdataï¼Œæ•°æ®é›†å‡†å¤‡è‡³FinEval/code/dataç›®å½•ä¸‹ã€‚

```
cd code
git clone *----------------
unzip xx.zip
mv xx data
```

dataæ–‡ä»¶å¤¹æ ¼å¼ä¸º:

- -----data
  - ----devï¼šæ¯ä¸ªç§‘ç›®çš„devé›†ä¸­åŒ…å«äº”ä¸ªç¤ºèŒƒå®ä¾‹ä»¥åŠfew-shotè¯„ä¼°æä¾›çš„è§£é‡Š
  - ----valï¼švalé›†ä¸»è¦ä½œç”¨äºè¶…å‚è°ƒæ•´
  - ----testï¼šç”¨äºæ¨¡å‹è¯„ä¼°ï¼Œtesté›†çš„æ ‡ç­¾ä¸ä¼šå…¬å¼€ï¼Œéœ€ç”¨æˆ·æäº¤å…¶ç»“æœï¼Œæ‰å¯ä»¥è·å¾—æµ‹è¯•å‡†ç¡®å€¼

## è¯„æµ‹

è¯·é˜…è¯»[å¿«é€Ÿä¸Šæ‰‹](/docs/zh_cn/get_started/quick_start.md)äº†è§£å¦‚ä½•è¿è¡Œä¸€ä¸ªè¯„æµ‹ä»»åŠ¡ã€‚

## æ”¯æŒæ–°æ•°æ®é›†å’Œæ¨¡å‹

å¦‚æœéœ€è¦æ–°åŠ å…¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹ï¼Œè¯·å‚è€ƒ[æ”¯æŒæ–°æ•°æ®é›†](/docs/zh_cn/advanced_guide/new_dataset.md)ã€‚

å¦‚æœéœ€è¦åŠ è½½æ–°æ¨¡å‹ï¼Œè¯·å‚è€ƒ[æ”¯æŒæ–°æ¨¡å‹](/docs/zh_cn/advanced_guide/new_model.md)ã€‚

## å¦‚ä½•æäº¤

æ‚¨é¦–å…ˆéœ€è¦å‡†å¤‡ä¸€ä¸ªUTF-8ç¼–ç çš„JSONæ–‡ä»¶ï¼Œå¹¶æŒ‰ç…§ä»¥ä¸‹æ ¼å¼ç¼–å†™ã€‚
```
## æ¯ä¸ªå­¦ç§‘å†…éƒ¨çš„é”®åæ˜¯æ•°æ®é›†ä¸­çš„"id"å­—æ®µ
{
    "banking_practitioner_qualification_certificate": {
        "0": "A",
        "1": "B",
        "2": "B",
        ...
    },
    
    "å­¦ç§‘åç§°":{
    "0":"ç­”æ¡ˆ1",
    "1":"ç­”æ¡ˆ2",
    ...
    }
    ....
}
```
ç„¶åä½ å¯ä»¥å°†å‡†å¤‡å¥½çš„JSONæ–‡ä»¶æäº¤åˆ°zhang.liwen@shufe.edu.cnã€‚

## å¼•ç”¨

```bibtex
@misc{2023opencompass,
    title={OpenCompass: A Universal Evaluation Platform for Foundation Models},
    author={OpenCompass Contributors},
    howpublished = {\url{https://github.com/InternLM/OpenCompass}},
    year={2023}
}
```
