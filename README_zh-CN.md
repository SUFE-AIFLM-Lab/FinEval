<div align="center">
  <img src="docs/zh_cn/_static/image/FinEval.jpg" width="500px"/>
  <br />
  <br />

[![license](https://img.shields.io/badge/License-Apache--2.0-blue.svg)](https://github.com/SUFE-AIFLM-Lab/FinEval/blob/main/LICENSE)

[ğŸŒç½‘ç«™](https://fineval.readthedocs.io/zh_CN/latest/) |
[ğŸ¤—Hugging Face](https://huggingface.co/datasets/SUFE-AIFLM-Lab/FinEval) |
[ğŸ“ƒè®ºæ–‡]()

[English](/README.md) | ç®€ä½“ä¸­æ–‡
</div>

æ¬¢è¿æ¥åˆ°**FinEval**

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œç„¶è€Œå®ƒä»¬åœ¨æ›´å…·æŒ‘æˆ˜æ€§å’Œé¢†åŸŸç‰¹å®šçš„ä»»åŠ¡ä¸­çš„åŠŸæ•ˆä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡ä»‹ç»äº†FinEvalï¼Œä¸€ä¸ªä¸“é—¨ä¸ºLLMsä¸­çš„é‡‘èé¢†åŸŸçŸ¥è¯†è€Œè®¾è®¡çš„åŸºå‡†æµ‹è¯•ã€‚

FinEvalæ˜¯ä¸€ä¸ªåŒ…å«é«˜è´¨é‡å¤šé¡¹é€‰æ‹©é¢˜çš„é›†åˆï¼Œæ¶µç›–**é‡‘èã€ç»æµã€ä¼šè®¡å’Œè¯ä¹¦**ç­‰é¢†åŸŸã€‚å®ƒåŒ…æ‹¬4,661ä¸ªé—®é¢˜ï¼Œæ¶µç›–äº†34ä¸ªä¸åŒçš„å­¦æœ¯ç§‘ç›®ã€‚ä¸ºäº†ç¡®ä¿å¯¹æ¨¡å‹æ€§èƒ½è¿›è¡Œå…¨é¢çš„è¯„ä¼°ï¼ŒFinEvalé‡‡ç”¨äº†å¤šç§æ–¹æ³•ï¼ŒåŒ…æ‹¬zero-shotï¼Œfew-shotï¼Œä»…é¢„æµ‹ç­”æ¡ˆï¼ˆanswer-onlyï¼‰å’Œæ€ç»´é“¾ï¼ˆchain-of-thoughtï¼‰æç¤ºè¯ã€‚é€šè¿‡åœ¨FinEvalä¸Šè¯„ä¼°æœ€å…ˆè¿›çš„ä¸­è‹±æ–‡å¤§è¯­è¨€æ¨¡å‹ï¼Œç»“æœæ˜¾ç¤ºåªæœ‰GPT-4åœ¨ä¸åŒæç¤ºè®¾ç½®ä¸‹è¾¾åˆ°äº†60%çš„å‡†ç¡®ç‡ï¼Œè¡¨æ˜å¤§è¯­è¨€æ¨¡å‹åœ¨é‡‘èé¢†åŸŸçŸ¥è¯†æ–¹é¢å…·æœ‰æ˜¾è‘—çš„å¢é•¿æ½œåŠ›ã€‚æˆ‘ä»¬çš„å·¥ä½œæä¾›äº†ä¸€ä¸ªæ›´å…¨é¢çš„é‡‘èçŸ¥è¯†è¯„ä¼°åŸºå‡†ï¼Œåˆ©ç”¨äº†æ¨¡æ‹Ÿè€ƒè¯•æ•°æ®ï¼Œæ¶µç›–äº†å¹¿æ³›çš„å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°èŒƒå›´ã€‚

<div align="center">
  <img src="docs/en/_static/image/subjects.png" width="1000px" height="475px"/>
  <br />
  <br /></div>

## ç›®å½•

- [æ€§èƒ½æ’è¡Œæ¦œ](#æ€§èƒ½æ’è¡Œæ¦œ)
- [å®‰è£…](#å®‰è£…)
- [è¯„æµ‹](#è¯„æµ‹)
- [å‡†å¤‡æ•°æ®é›†](#å‡†å¤‡æ•°æ®é›†)
- [æ”¯æŒæ–°æ•°æ®é›†å’Œæ¨¡å‹](#æ”¯æŒæ–°æ•°æ®é›†å’Œæ¨¡å‹)
- [å¦‚ä½•æäº¤](#å¦‚ä½•æäº¤)
- [å¼•ç”¨](#å¼•ç”¨)


## æ€§èƒ½æ’è¡Œæ¦œ

æˆ‘ä»¬åˆ†ä¸º**ä»…é¢„æµ‹ç­”æ¡ˆ**å’Œ**æ€ç»´é“¾**å¯¹æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œå¦‚æœéœ€è¦äº†è§£ä¸¤ç§æ–¹æ³•çš„Promptæ ·ä¾‹ï¼Œè¯·å‚è€ƒ[ä»…é¢„æµ‹ç­”æ¡ˆçš„zero-shot](/docs/zh_cn/prompt/zero_shot.md)ã€[ä»…é¢„æµ‹ç­”æ¡ˆçš„few-shot](/docs/zh_cn/prompt/few_shot.md)å’Œ[æ€ç»´é“¾](/docs/zh_cn/prompt/cot.md)ã€‚

ä¸‹é¢æ˜¯æˆ‘ä»¬è¯„ä¼°æ¨¡å‹åœ¨æµ‹è¯•é›†ï¼ˆtestï¼‰ä¸Šçš„å¹³å‡å‡†ç¡®ç‡(%)ã€‚æ¯ä¸ªç±»åˆ«ä¸‹çš„å¹³å‡å‡†ç¡®ç‡æ˜¯è¯¥ç±»åˆ«ä¸‹æ‰€æœ‰å­¦ç§‘çš„å¹³å‡å‡†ç¡®ç‡ï¼Œæœ€åä¸€åˆ—æ˜¯æ¨¡å‹åœ¨æ‰€æœ‰å­¦ç§‘ä¸Šçš„å¹³å‡å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œåœ¨å››ç§Promptè®¾ç½®ä¸‹ï¼Œæˆ‘ä»¬åªç»™å‡ºäº†æ‰€æœ‰å­¦ç§‘å¹³å‡å‡†ç¡®ç‡æœ€é«˜çš„è®¾ç½®ç»“æœã€‚

| Model                  | Size    | Finance | Economy | Accounting | Certificate | Average |
|------------------------|---------|:-------:|:-------:|:----------:|:-----------:|:-------:|
| GPT-4                  | unknown |  71.0   |  74.5   |    59.3    |    70.4     |  68.6   |
| ChatGPT                | 175B    |  59.3   |  61.6   |    45.2    |    55.1     |  55.0   |
| Qwen-7B                | 7B      |  54.5   |  54.4   |    50.3    |    55.8     |  53.8   |
| Qwen-Chat-7B           | 7B      |  51.5   |  52.1   |    44.5    |    53.6     |  50.5   |
| Baichuan-13B-Base      | 13B     |  52.6   |  50.2   |    43.4    |    53.5     |  50.1   |
| Baichuan-13B-Chat      | 13B     |  51.6   |  51.1   |    41.7    |    52.8     |  49.4   |
| ChatGLM2-6B            | 6B      |  46.5   |  46.4   |    44.5    |    51.5     |  47.4   |
| InternLM-7B            | 7B      |  49.0   |  49.2   |    40.5    |    49.4     |  47.1   |
| InternLM-Chat-7B       | 7B      |  48.4   |  49.1   |    40.8    |    49.5     |  47.0   |
| LLaMA-2-Chat-70B       | 70B     |  47.1   |  46.7   |    41.5    |    45.7     |  45.2   |
| Falcon-40B             | 40B     |  45.4   |  43.2   |    35.8    |    44.8     |  42.4   |
| Baichuan-7B            | 7B      |  44.9   |  41.5   |    34.9    |    45.6     |  42.0   |
| LLaMA-2-Chat-13B       | 13B     |  41.6   |  38.4   |    34.1    |    42.1     |  39.3   |
| Ziya-LLaMA-13B-v1      | 13B     |  43.3   |  36.9   |    34.3    |    41.2     |  39.3   |
| Bloomz-7b1-mt          | 7B      |  41.4   |  42.1   |    32.5    |    39.7     |  38.8   |
| LLaMA-2-13B            | 13B     |  39.5   |  38.6   |    31.6    |    39.6     |  37.4   |
| ChatGLM-6B             | 6B      |  38.8   |  36.2   |    33.8    |    39.1     |  37.2   |
| Chinese-Llama-2-7B     | 7B      |  37.8   |  37.8   |    31.4    |    36.7     |  35.9   |
| Chinese-Alpaca-Plus-7B | 7B      |  30.5   |  33.4   |    32.7    |    38.5     |  34.0   |
| moss-moon-003-sft      | 16B     |  35.6   |  34.3   |    28.7    |    35.6     |  33.7   |
| LLaMA-2-Chat-7B        | 7B      |  35.6   |  31.8   |    31.9    |    34.0     |  33.5   |
| LLaMA-2-7B             | 7B      |  34.9   |  36.4   |    31.4    |    31.6     |  33.4   |
| AquilaChat-7B          | 7B      |  34.2   |  31.3   |    29.8    |    36.2     |  33.1   |
| moss-moon-003-base     | 16B     |  32.2   |  33.1   |    29.2    |    30.7     |  31.2   |
| Aquila-7B              | 7B      |  27.1   |  31.6   |    32.4    |    33.6     |  31.2   |
| LLaMA-13B              | 13B     |  33.1   |  29.7   |    27.2    |    33.6     |  31.1   |
| Falcon-7B              | 7B      |  28.5   |  28.2   |    27.5    |    27.4     |  27.9   |


## å®‰è£…

ä¸‹é¢å±•ç¤ºäº†å¿«é€Ÿå®‰è£…çš„æ­¥éª¤ï¼Œè¯¦ç»†è¯·å‚è€ƒ[å®‰è£…æŒ‡å—](docs/zh_cn/get_started/install.md)ã€‚

 ```python
    conda create --name fineval_venv python=3.8
    conda activate fineval_venv
 ```

```python
    git clone https://github.com/SUFE-AIFLM/FinEval
    cd FinEval
    pip install -r requirements.txt
    
    requirements.txt æ–‡ä»¶å¦‚ä¸‹:
    pandas
    numpy
    torch
    tqdm
    peft 
    sentencepiece
    openai
    accelerate
    colorama
    cpm_kernels
    sentencepiece
    streamlit
    transformers_stream_generator
    transformers==4.31.0
    tiktoken
    einops
    scipy
```

## å‡†å¤‡æ•°æ®é›†

ä½¿ç”¨Hugging Face datasetsä¸‹è½½æ•°æ®é›†ã€‚è¿è¡Œå‘½ä»¤è¿›è¡Œ**æ‰‹åŠ¨ä¸‹è½½è§£å‹**ï¼Œåœ¨Fineval/codeçš„é¡¹ç›®ç›®å½•ä¸‹è¿è¡Œä¸‹é¢å‘½ä»¤ï¼Œå¹¶æ”¹åä¸ºdataï¼Œæ•°æ®é›†å‡†å¤‡è‡³FinEval/code/dataç›®å½•ä¸‹ã€‚

```
cd code/data
wget https://huggingface.co/datasets/SUFE-AIFLM-Lab/FinEval/resolve/main/FinEval.zip
unzip FinEval.zip
```

dataæ–‡ä»¶å¤¹æ ¼å¼ä¸º:

- -----data
  - ----devï¼šæ¯ä¸ªç§‘ç›®çš„devé›†ä¸­åŒ…å«äº”ä¸ªç¤ºèŒƒå®ä¾‹ä»¥åŠfew-shotè¯„ä¼°æä¾›çš„è§£é‡Š
  - ----valï¼švalé›†ä¸»è¦ä½œç”¨äºè¶…å‚è°ƒæ•´
  - ----testï¼šç”¨äºæ¨¡å‹è¯„ä¼°ï¼Œtesté›†çš„æ ‡ç­¾ä¸ä¼šå…¬å¼€ï¼Œéœ€ç”¨æˆ·æäº¤å…¶ç»“æœï¼Œæ‰å¯ä»¥è·å¾—æµ‹è¯•å‡†ç¡®å€¼

## è¯„æµ‹

è¯·é˜…è¯»[å¿«é€Ÿä¸Šæ‰‹](/docs/zh_cn/get_started/quick_start.md)äº†è§£å¦‚ä½•è¿è¡Œä¸€ä¸ªè¯„æµ‹ä»»åŠ¡ã€‚

## æ”¯æŒæ–°æ•°æ®é›†å’Œæ¨¡å‹

å¦‚æœéœ€è¦æ–°åŠ å…¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹ï¼Œè¯·å‚è€ƒ[æ”¯æŒæ–°æ•°æ®é›†](/docs/zh_cn/advanced_guide/new_dataset.md)ã€‚

å¦‚æœéœ€è¦åŠ è½½æ–°æ¨¡å‹ï¼Œè¯·å‚è€ƒ[æ”¯æŒæ–°æ¨¡å‹](/docs/zh_cn/advanced_guide/new_model.md)ã€‚

## å¦‚ä½•æäº¤

æ‚¨é¦–å…ˆéœ€è¦å‡†å¤‡ä¸€ä¸ªUTF-8ç¼–ç çš„JSONæ–‡ä»¶ï¼Œå¹¶æŒ‰ç…§ä»¥ä¸‹æ ¼å¼ç¼–å†™ã€‚
```
## æ¯ä¸ªå­¦ç§‘å†…éƒ¨çš„é”®åæ˜¯æ•°æ®é›†ä¸­çš„"id"å­—æ®µ
{
    "banking_practitioner_qualification_certificate": {
        "0": "A",
        "1": "B",
        "2": "B",
        ...
    },
    
    "å­¦ç§‘åç§°":{
    "0":"ç­”æ¡ˆ1",
    "1":"ç­”æ¡ˆ2",
    ...
    }
    ....
}
```
ç„¶åä½ å¯ä»¥å°†å‡†å¤‡å¥½çš„JSONæ–‡ä»¶æäº¤åˆ°zhang.liwen@shufe.edu.cnã€‚

## å¼•ç”¨

```bibtex
@misc{2023opencompass,
    title={OpenCompass: A Universal Evaluation Platform for Foundation Models},
    author={OpenCompass Contributors},
    howpublished = {\url{https://github.com/InternLM/OpenCompass}},
    year={2023}
}
```
